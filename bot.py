# ==========================================
# å…§æ¹–é«˜å·¥å®¶é•·å°å¹«æ‰‹ (è‡ªå‹•èœ˜è››ç‰ˆ)
# ==========================================
import os
import logging
import asyncio
import nest_asyncio
import requests                   # æ–°å¢: ç”¨ä¾†æŠ“ç¶²é é€£çµ
from bs4 import BeautifulSoup     # æ–°å¢: ç”¨ä¾†åˆ†æ HTML
from urllib.parse import urljoin  # æ–°å¢: ç”¨ä¾†è™•ç†ç¶²å€æ‹¼æ¥
from flask import Flask, request, abort

# --- LINE SDK v3 ---
from linebot.v3 import WebhookHandler
from linebot.v3.exceptions import InvalidSignatureError
from linebot.v3.messaging import (
    Configuration, ApiClient, MessagingApi, ReplyMessageRequest, TextMessage
)
from linebot.v3.webhooks import MessageEvent, TextMessageContent

# --- AI èˆ‡ è³‡æ–™åº« ---
from crawl4ai import AsyncWebCrawler
import google.generativeai as genai
import chromadb

nest_asyncio.apply()
logging.basicConfig(level=logging.INFO)

app = Flask(__name__)

# ğŸ”‘ è¨­å®šå€ (å»ºè­°åœ¨ Render å¾Œå°è¨­å®šç’°å¢ƒè®Šæ•¸)
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY", "æ‚¨çš„_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN", "æ‚¨çš„_TOKEN")
LINE_CHANNEL_SECRET = os.environ.get("LINE_CHANNEL_SECRET", "æ‚¨çš„_SECRET")

# åˆå§‹åŒ–
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash')
configuration = Configuration(access_token=LINE_CHANNEL_ACCESS_TOKEN)
handler = WebhookHandler(LINE_CHANNEL_SECRET)

# è³‡æ–™åº« (Render é‡é–‹æ©Ÿå¾Œæœƒé‡ç½®)
chroma_client = chromadb.Client()
collection = chroma_client.get_or_create_collection(name="school_data_spider")

# ==========================================
# ğŸ•·ï¸ èœ˜è››åŠŸèƒ½: è‡ªå‹•å°‹æ‰¾é‡è¦é€£çµ
# ==========================================
def get_school_links(start_url, max_limit=15):
    """
    å¾é¦–é å‡ºç™¼ï¼ŒæŠ“å–å‰ max_limit å€‹ä¸é‡è¤‡çš„å­¸æ ¡å…§ç¶²é€£çµ
    """
    print(f"ğŸ•·ï¸ [èœ˜è››] æ­£åœ¨æƒæé¦–é : {start_url}")
    found_urls = set()  # ä½¿ç”¨ set (é›†åˆ) è‡ªå‹•å»é™¤é‡è¤‡ç¶²å€
    
    # 1. ç‚ºäº†ç¢ºä¿é‡è¦è³‡è¨Šä¸è¢«éºæ¼ï¼Œæˆ‘å€‘å…ˆæ‰‹å‹•åŠ å…¥å¿…å‚™ç¶²å€
    important_urls = [
        "https://www.nihs.tp.edu.tw/nss/s/principal/p/01", # æ ¡é•·ç°¡ä»‹
        "https://www.nihs.tp.edu.tw/nss/p/contact",        # è¯çµ¡è³‡è¨Š
        "https://www.nihs.tp.edu.tw/nss/p/access",         # äº¤é€šè³‡è¨Š
        start_url                                          # é¦–é è‡ªå·±
    ]
    for url in important_urls:
        found_urls.add(url)

    # 2. è‡ªå‹•åˆ†æé¦–é é€£çµ
    try:
        response = requests.get(start_url, timeout=5)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        links = soup.find_all('a', href=True)
        print(f"ğŸ•¸ï¸ [èœ˜è››] é¦–é å…±ç™¼ç¾ {len(links)} å€‹é€£çµï¼Œæ­£åœ¨éæ¿¾...")

        for link in links:
            if len(found_urls) >= max_limit:
                break
                
            href = link['href']
            full_url = urljoin(start_url, href)
            
            # éæ¿¾è¦å‰‡:
            # 1. å¿…é ˆæ˜¯å­¸æ ¡ç¶²åŸŸ (é˜²æ­¢çˆ¬åˆ°å¤–éƒ¨å»£å‘Š)
            # 2. æ’é™¤æª”æ¡ˆä¸‹è¼‰ (jpg, pdf, zip) é¿å…çˆ¬èŸ²å¡ä½
            # 3. æ’é™¤ç™»å…¥é é¢
            if "nihs.tp.edu.tw" in full_url:
                if not any(x in full_url.lower() for x in ['.jpg', '.png', '.pdf', '.zip', '.doc', 'login', 'passport']):
                    found_urls.add(full_url)
                    
    except Exception as e:
        print(f"âš ï¸ [èœ˜è››] æƒæå¤±æ•—: {e}")

    final_list = list(found_urls)
    print(f"âœ… [èœ˜è››] æœ€çµ‚ç¢ºèªçˆ¬å–ç›®æ¨™: {len(final_list)} é ")
    return final_list

# é»ƒé‡‘å°æŠ„ (å¿…å‚™çŸ¥è­˜)
school_fact_sheet = """
ã€å­¸æ ¡åŸºæœ¬è³‡æ–™ã€‘
* ç¾ä»»æ ¡é•·ï¼šæ—ä¿Šå²³
* å­¸æ ¡åœ°å€ï¼šè‡ºåŒ—å¸‚å…§æ¹–å€å…§æ¹–è·¯ä¸€æ®µ520è™Ÿ
* ç¸½æ©Ÿé›»è©±ï¼š(02) 2657-4874
* å­¸æ ¡ç¶²å€ï¼šhttps://www.nihs.tp.edu.tw
"""

# ==========================================
# ğŸ§  çŸ¥è­˜åº«æ›´æ–° (å«éæ¿¾æ©Ÿåˆ¶)
# ==========================================
async def update_knowledge_base():
    # 1. å•Ÿå‹•èœ˜è››æŠ“é€£çµ
    target_urls = get_school_links("https://www.nihs.tp.edu.tw/nss/p/index", max_limit=15)
    
    print("ğŸš€ [çˆ¬èŸ²] é–‹å§‹è®€å–ç¶²é å…§å®¹...")
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun_many(urls=target_urls)

    success_count = 0
    for result in results:
        if not result.success: continue
        
        content = result.markdown
        # ğŸ§¹ é›œè¨Šéæ¿¾: å…§å®¹å¤ªçŸ­é€šå¸¸æ˜¯ç„¡æ•ˆé é¢
        if not content or len(content) < 50: 
            print(f"ğŸ—‘ï¸ [éæ¿¾] å…§å®¹éçŸ­ï¼Œè·³é: {result.url}")
            continue
            
        # åˆ‡å‰²ä¸¦å­˜å…¥è³‡æ–™åº«
        chunk_size = 1000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        
        # é€™è£¡ä½¿ç”¨ url + åºè™Ÿä½œç‚º IDï¼Œç¢ºä¿åŒä¸€ç¶²å€é‡è·‘æ™‚æœƒè¦†è“‹èˆŠè³‡æ–™ï¼Œä¸æœƒé‡è¤‡å †ç–Š
        if chunks:
            ids = [f"{result.url}_{i}" for i in range(len(chunks))]
            collection.upsert(
                documents=chunks,
                ids=ids,
                metadatas=[{"source": result.url} for _ in range(len(chunks))]
            )
            success_count += 1
            
    print(f"âœ… [å®Œæˆ] æˆåŠŸå»ºç«‹ {success_count} å€‹é é¢çš„çŸ¥è­˜åº«ï¼")

# ==========================================
# ğŸ¤– LINE Webhook èˆ‡ å•Ÿå‹•
# ==========================================
@app.route("/callback", methods=['POST'])
def callback():
    signature = request.headers['X-Line-Signature']
    body = request.get_data(as_text=True)
    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        abort(400)
    return 'OK'

@handler.add(MessageEvent, message=TextMessageContent)
def handle_message(event):
    user_msg = event.message.text
    try:
        # æœå°‹è³‡æ–™åº«
        results = collection.query(query_texts=[user_msg], n_results=3)
        context_text = ""
        if results['documents'] and results['documents'][0]:
            for i, doc in enumerate(results['documents'][0]):
                context_text += f"ã€ä¾†æºã€‘: {results['metadatas'][0][i]['source']}\n{doc[:200]}...\n\n"

        if not context_text:
            prompt = f"è«‹åƒè€ƒåŸºæœ¬è³‡æ–™å›ç­”ï¼š{school_fact_sheet}\nä½¿ç”¨è€…å•é¡Œï¼š{user_msg}"
        else:
            prompt = f"""
            ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å®¶é•·å°å¹«æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹è³‡æ–™å›ç­”å•é¡Œã€‚
            
            ã€å­¸æ ¡åŸºæœ¬è³‡æ–™ (æœ€å„ªå…ˆ)ã€‘ï¼š
            {school_fact_sheet}
            
            ã€æœå°‹åˆ°çš„ç¶²é è³‡æ–™ã€‘ï¼š
            {context_text}
            
            ã€ä½¿ç”¨è€…å•é¡Œã€‘ï¼š{user_msg}
            
            å›ç­”æ™‚è«‹é™„ä¸Šè³‡æ–™ä¾†æºç¶²å€ã€‚
            """

        response = model.generate_content(prompt)
        with ApiClient(configuration) as api_client:
            line_bot_api = MessagingApi(api_client)
            line_bot_api.reply_message(
                ReplyMessageRequest(
                    reply_token=event.reply_token,
                    messages=[TextMessage(text=response.text)]
                )
            )
    except Exception as e:
        print(f"Error: {e}")

# å•Ÿå‹•æ™‚åŸ·è¡Œçˆ¬èŸ²
with app.app_context():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(update_knowledge_base())

if __name__ == "__main__":
    app.run()
