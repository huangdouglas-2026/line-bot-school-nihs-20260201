# ====================================================
# ğŸ›ï¸ éœæ…‹é é¢æ•æ‰‹ V43 (Recursive Navigator / éè¿´å°èˆªç‰ˆ)
# ç›®æ¨™ï¼š
# 1. æŠ“å–ä¸»é¸å–®é é¢
# 2. è‡ªå‹•åµæ¸¬å·¦å´å­é¸å–® (nav-Vertical) ä¸¦éè¿´æŠ“å–
# 3. æ’é™¤å…¬å‘Šåˆ—è¡¨ï¼ŒåªæŠ“å–éœæ…‹å…§å®¹ (htmldisplay)
# ====================================================
import asyncio
import json
import os
from datetime import datetime
from playwright.async_api import async_playwright

# ğŸ“‚ è¨­å®š
OUTPUT_FILENAME = "nihs_static_data_v43.json"
BASE_URL = "https://www.nihs.tp.edu.tw/nss/p/"
MAX_DEPTH = 3  # è¨­å®šéè¿´æ·±åº¦é™åˆ¶

# åˆå§‹ç›®æ¨™ (ä¸»é¸å–®)
START_PAGES = {
    # --- é—œæ–¼æ¹–å·¥ ---
    "é—œæ–¼æ¹–å·¥-æœ¬æ ¡ç·£èµ·": "21",
    "é—œæ–¼æ¹–å·¥-å„ªè³ªç’°å¢ƒ": "23",
    "é—œæ–¼æ¹–å·¥-çµ„ç¹”æ¶æ§‹": "22",
    "é—œæ–¼æ¹–å·¥-æ¥­å‹™è·æŒ": "org1",
    "é—œæ–¼æ¹–å·¥-å¤§äº‹ç´€": "210",

    # --- è¡Œæ”¿å–®ä½ ---
    "è¡Œæ”¿å–®ä½-æ ¡é•·å®¤": "headmaster1",
    "è¡Œæ”¿å–®ä½-æ•™å‹™è™•": "32",
    "è¡Œæ”¿å–®ä½-å­¸å‹™è™•": "student9",
    "è¡Œæ”¿å–®ä½-å¯¦ç¿’è™•": "practice1",
    "è¡Œæ”¿å–®ä½-åœ–æ›¸é¤¨": "library03",
    "è¡Œæ”¿å–®ä½-ç¸½å‹™è™•": "36",
    "è¡Œæ”¿å–®ä½-è¼”å°å®¤": "37",
    "è¡Œæ”¿å–®ä½-äººäº‹å®¤": "39",
    "è¡Œæ”¿å–®ä½-æœƒè¨ˆå®¤": "310",

    # --- æ•™å­¸å–®ä½ (ç§‘ç³»ä»‹ç´¹) ---
    "æ•™å­¸å–®ä½-å…±åŒç§‘ç›®": "48",
    "æ•™å­¸å–®ä½-é›»å­ç§‘": "42",
    "æ•™å­¸å–®ä½-é›»æ©Ÿç§‘": "41",
    "æ•™å­¸å–®ä½-è³‡è¨Šç§‘": "44",
    "æ•™å­¸å–®ä½-æ§åˆ¶ç§‘": "con",
    "æ•™å­¸å–®ä½-å†·å‡ç©ºèª¿ç§‘": "43",
    "æ•™å­¸å–®ä½-æ‡‰ç”¨è‹±èªç§‘": "46",
    "æ•™å­¸å–®ä½-é–€å¸‚æœå‹™ç§‘": "47",
    "æ•™å­¸å–®ä½-å®¶é›»æŠ€è¡“ç§‘": "49",

    # --- å­¸ç”Ÿåœ’åœ° (åƒ…æŠ“å–æ ¡å…§éœæ…‹é ï¼Œæ’é™¤å¤–éƒ¨ç³»çµ±) ---
    "å­¸ç”Ÿåœ’åœ°-å­¸ç”Ÿæ‰‹å†Š": "stuhb",
    
    # --- ç›¸é—œçµ„ç¹” ---
    "ç›¸é—œçµ„ç¹”-æ•™å¸«æœƒ": "teacher",
    "ç›¸é—œçµ„ç¹”-å®¶é•·æœƒ": "92",
    "ç›¸é—œçµ„ç¹”-åˆä½œç¤¾": "93",
    "ç›¸é—œçµ„ç¹”-å¤¥ä¼´å­¸æ ¡": "76",
    
    # --- English ---
    "English-History & Features": "english2",
    "English-Department Profile": "english3"
}

# ç”¨ä¾†è¨˜éŒ„å·²æŠ“éçš„ç¶²å€ï¼Œé¿å…ç„¡çª®è¿´åœˆ
visited_urls = set()
all_data = []

async def extract_content(page, category, title, url, depth=0):
    """æŠ“å–å–®ä¸€é é¢çš„éœæ…‹å…§å®¹"""
    if url in visited_urls: return
    if depth > MAX_DEPTH: return # è¶…éæ·±åº¦é™åˆ¶å‰‡åœæ­¢

    visited_urls.add(url)
    
    prefix = "  " * depth # ç¸®æ’é¡¯ç¤ºå±¤ç´š
    print(f"{prefix}ğŸ” åˆ†æé é¢: [{category}] {title} ...")
    
    data = {
        "category": "æ ¡åœ’éœæ…‹è³‡è¨Š",
        "unit": category,
        "date": datetime.now().strftime("%Y/%m/%d"),
        "title": title,
        "url": url,
        "content": "",
        "attachments": [],
        "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    try:
        # è¨­å®šè¼ƒé•·çš„ timeout é¿å…ç¶²è·¯æ…¢æ™‚éŒ¯èª¤
        await page.goto(url, timeout=60000, wait_until='domcontentloaded')
        
        # ç­‰å¾…ä¸»è¦å…§å®¹æˆ–é¸å–®å‡ºç¾ï¼Œå®¹éŒ¯è™•ç†
        try:
            await page.wait_for_selector(".module-content", timeout=5000)
        except: 
            pass # æ²’æ‰¾åˆ°ä¹Ÿæ²’é—œä¿‚ï¼Œç¹¼çºŒå˜—è©¦æŠ“å…§å®¹
        
        await page.wait_for_timeout(1000) # ç­‰å¾…æ¸²æŸ“

        # --- A. æŠ“å–éœæ…‹å…§å®¹ (æ’é™¤å…¬å‘Šåˆ—è¡¨) ---
        # æˆ‘å€‘åªå° .htmldisplay æ„Ÿèˆˆè¶£ (é€™æ˜¯ Ischool ç³»çµ±æ”¾éœæ…‹åœ–æ–‡çš„åœ°æ–¹)
        # å°‹æ‰¾æ‰€æœ‰éœæ…‹å€å¡Šï¼Œä½†æ’é™¤åŒ…å«å…¬å‘Šåˆ—è¡¨çš„å€å¡Š
        content_blocks = await page.locator(".htmldisplay").all()
        
        full_text = ""
        for block in content_blocks:
            # ç¢ºä¿é€™å€‹å€å¡Šå¯è¦‹
            if await block.is_visible():
                text = await block.inner_text()
                full_text += text + "\n"
                
                # æŠ“å–è©²å€å¡Šå…§çš„é™„ä»¶
                links = await block.locator("a").all()
                for link in links:
                    href = await link.get_attribute("href")
                    name = await link.inner_text()
                    if href and any(ext in href.lower() for ext in ['.pdf', '.doc', '.xls', '.ppt', '.jpg', '.png']):
                        if href.startswith("/"): href = "https://www.nihs.tp.edu.tw" + href
                        data["attachments"].append({"name": name.strip(), "url": href})
        
        # ç°¡å–®æ¸…æ´—
        data["content"] = "\n".join([line.strip() for line in full_text.split('\n') if line.strip()])
        
        if data["content"]:
            print(f"{prefix}   ğŸ“ æŠ“åˆ°å…§å®¹: {len(data['content'])} å­—")
            all_data.append(data)
        else:
            print(f"{prefix}   âš ï¸ ç„¡éœæ…‹å…§æ–‡ (å¯èƒ½åƒ…æ˜¯ç›®éŒ„é )")

        # --- B. åµæ¸¬å­é¸å–® (éè¿´æ ¸å¿ƒ) ---
        # å°‹æ‰¾å·¦å´å°èˆªåˆ— (.nav-Vertical a)
        sub_links = await page.locator(".nav-Vertical a").all()
        
        # æ”¶é›†éœ€è¦å‰å¾€çš„å­é€£çµ
        next_targets = []
        for link in sub_links:
            href = await link.get_attribute("href")
            name = await link.inner_text()
            name = name.strip()
            
            if href and name:
                # æ’é™¤å¤–éƒ¨é€£çµ (httpé–‹é ­ä½†ä¸æ˜¯æœ¬æ ¡)
                if href.startswith("http") and "nihs.tp.edu.tw" not in href: continue
                
                # è™•ç†ç›¸å°è·¯å¾‘ (Ischool ç³»çµ±é€šå¸¸ç›´æ¥çµ¦ PageIDï¼Œä¾‹å¦‚ "Academic2")
                if not href.startswith("http"):
                    # åˆ¤æ–·æ˜¯å¦å·²ç¶“åŒ…å« /nss/p/ï¼Œé¿å…é‡è¤‡ç–ŠåŠ 
                    if href.startswith("/nss/p/"):
                         full_href = f"https://www.nihs.tp.edu.tw{href}"
                    elif href.startswith("/"): # å…¶ä»–æ ¹ç›®éŒ„é€£çµ
                         full_href = f"https://www.nihs.tp.edu.tw{href}"
                    else: # ç´” PageID
                         full_href = f"{BASE_URL}{href}"
                else:
                    full_href = href
                
                # å¦‚æœé‚„æ²’æŠ“éï¼Œå°±åŠ å…¥ä½‡åˆ—
                if full_href not in visited_urls:
                    next_targets.append((name, full_href))

        # éè¿´æŠ“å–å­é é¢
        if next_targets:
            print(f"{prefix}   ğŸ”— ç™¼ç¾ {len(next_targets)} å€‹å­åˆ†é ï¼Œæº–å‚™æ·±å…¥...")
            for sub_name, sub_url in next_targets:
                # éè¿´å‘¼å« (å‚³éç•¶å‰çš„ category ä½œç‚ºæ¯å–®ä½)
                await extract_content(page, category, f"{title}-{sub_name}", sub_url, depth + 1)
                await page.wait_for_timeout(500)

    except Exception as e:
        print(f"{prefix}   âŒ è™•ç†å¤±æ•—: {e}")

async def main():
    print("ğŸš€ V43 (éè¿´å°èˆªç‰ˆ) å•Ÿå‹•...")
    async with async_playwright() as p:
        # âš ï¸ é‡è¦ä¿®æ­£ï¼šé›²ç«¯ç’°å¢ƒé€šå¸¸éœ€è¦ headless=Trueï¼Œæœ¬åœ°æ¸¬è©¦å¯æ”¹ç‚º False
        # ç‚ºäº†ä¿éšªèµ·è¦‹ï¼Œæˆ‘å€‘é è¨­ True (èƒŒæ™¯åŸ·è¡Œ)ï¼Œé€™æ¨£åœ¨ GitHub Actions å°±ä¸æœƒå ±éŒ¯
        browser = await p.chromium.launch(headless=True) 
        
        # è¨­å®š User-Agent æ¨¡æ“¬çœŸå¯¦ç€è¦½å™¨ï¼Œé¿å…è¢«æ“‹
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
        )
        page = await context.new_page()

        # å¾è¨­å®šçš„ç¨®å­é é¢é–‹å§‹
        for name, pid in START_PAGES.items():
            start_url = f"{BASE_URL}{pid}"
            # å¾é ‚å±¤é–‹å§‹æŠ“
            await extract_content(page, name, name, start_url)

        await browser.close()

    # å­˜æª”
    print("\n" + "="*30)
    if len(all_data) > 0:
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=4)
        print(f"âœ… å®Œæˆï¼å…±æŠ“å– {len(all_data)} å€‹éœæ…‹é é¢ã€‚")
        print(f"ğŸ‘‰ æª”æ¡ˆä½ç½®: {os.path.abspath(OUTPUT_FILENAME)}")
    else:
        print("âš ï¸ æœªæŠ“å–åˆ°è³‡æ–™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
