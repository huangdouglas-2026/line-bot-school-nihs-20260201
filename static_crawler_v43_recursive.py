# ====================================================
# ğŸ›ï¸ éœæ…‹é é¢æ•æ‰‹ V44 (å¯¬å®¹æ“·å– + æ™ºæ…§ç­‰å¾…ç‰ˆ)
# ====================================================
import asyncio
import json
import os
import random
from datetime import datetime
from playwright.async_api import async_playwright

# ğŸ“‚ è¨­å®š
OUTPUT_FILENAME = "nihs_static_data_v43.json" # ç¶­æŒ v43 æª”åä»¥ä¾¿ merge_data è®€å–
BASE_URL = "https://www.nihs.tp.edu.tw/nss/p/"
MAX_DEPTH = 3 

# åˆå§‹ç›®æ¨™ (ç¶­æŒä¸è®Š)
START_PAGES = {
    "é—œæ–¼æ¹–å·¥-æœ¬æ ¡ç·£èµ·": "21",
    "é—œæ–¼æ¹–å·¥-å„ªè³ªç’°å¢ƒ": "23",
    "é—œæ–¼æ¹–å·¥-çµ„ç¹”æ¶æ§‹": "22",
    "é—œæ–¼æ¹–å·¥-æ¥­å‹™è·æŒ": "org1",
    "é—œæ–¼æ¹–å·¥-å¤§äº‹ç´€": "210",
    "è¡Œæ”¿å–®ä½-æ ¡é•·å®¤": "headmaster1",
    "è¡Œæ”¿å–®ä½-æ•™å‹™è™•": "32",
    "è¡Œæ”¿å–®ä½-å­¸å‹™è™•": "student9",
    "è¡Œæ”¿å–®ä½-å¯¦ç¿’è™•": "practice1",
    "è¡Œæ”¿å–®ä½-åœ–æ›¸é¤¨": "library03",
    "è¡Œæ”¿å–®ä½-ç¸½å‹™è™•": "36",
    "è¡Œæ”¿å–®ä½-è¼”å°å®¤": "37",
    "è¡Œæ”¿å–®ä½-äººäº‹å®¤": "39",
    "è¡Œæ”¿å–®ä½-æœƒè¨ˆå®¤": "310",
    "æ•™å­¸å–®ä½-å…±åŒç§‘ç›®": "48",
    "æ•™å­¸å–®ä½-é›»å­ç§‘": "42",
    "æ•™å­¸å–®ä½-é›»æ©Ÿç§‘": "41",
    "æ•™å­¸å–®ä½-è³‡è¨Šç§‘": "44",
    "æ•™å­¸å–®ä½-æ§åˆ¶ç§‘": "con",
    "æ•™å­¸å–®ä½-å†·å‡ç©ºèª¿ç§‘": "43",
    "æ•™å­¸å–®ä½-æ‡‰ç”¨è‹±èªç§‘": "46",
    "æ•™å­¸å–®ä½-é–€å¸‚æœå‹™ç§‘": "47",
    "æ•™å­¸å–®ä½-å®¶é›»æŠ€è¡“ç§‘": "49",
    "å­¸ç”Ÿåœ’åœ°-å­¸ç”Ÿæ‰‹å†Š": "stuhb",
    "ç›¸é—œçµ„ç¹”-æ•™å¸«æœƒ": "teacher",
    "ç›¸é—œçµ„ç¹”-å®¶é•·æœƒ": "92",
    "ç›¸é—œçµ„ç¹”-åˆä½œç¤¾": "93",
    "ç›¸é—œçµ„ç¹”-å¤¥ä¼´å­¸æ ¡": "76"
}

visited_urls = set()
all_data = []

async def extract_content(page, category, title, url, depth=0):
    if url in visited_urls: return
    if depth > MAX_DEPTH: return 

    visited_urls.add(url)
    
    prefix = "  " * depth
    print(f"{prefix}ğŸ” åˆ†æé é¢: [{category}] {title}")
    
    data = {
        "category": "æ ¡åœ’éœæ…‹è³‡è¨Š",
        "unit": category,
        "date": datetime.now().strftime("%Y/%m/%d"),
        "title": title,
        "url": url,
        "content": "",
        "attachments": [],
        "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    try:
        # âœ… ä¿®æ­£ 1: å»¶é•· Timeout ä¸¦ä½¿ç”¨ networkidle (ç­‰å¾…ç¶²è·¯éœæ­¢)
        # GitHub Actions æ¯”è¼ƒæ…¢ï¼Œçµ¦å®ƒå¤šä¸€é»æ™‚é–“
        await page.goto(url, timeout=60000, wait_until='domcontentloaded')
        
        try:
            # å˜—è©¦ç­‰å¾…ç¶²è·¯é–’ç½® (æœ€æº–ç¢ºï¼Œä½†æœ‰æ™‚æœƒç­‰å¤ªä¹…ï¼Œè¨­å€‹ timeout)
            await page.wait_for_load_state("networkidle", timeout=5000)
        except:
            pass # å¦‚æœè¶…æ™‚å°±ä¸ç­‰äº†ï¼Œç¹¼çºŒå¾€ä¸‹

        # âœ… ä¿®æ­£ 2: æ¨¡æ“¬äººé¡æ²å‹• (è§¸ç™¼ Lazy Loading å…§å®¹)
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        await page.wait_for_timeout(2000) # å†çµ¦ 2 ç§’è®“å…§å®¹é•·å‡ºä¾†

        # --- A. æŠ“å–å…§å®¹ (å¤šé‡ç­–ç•¥) ---
        full_text = ""
        
        # ç­–ç•¥ 1: å„ªå…ˆæŠ“å–æ¨™æº–éœæ…‹å€å¡Š (.htmldisplay)
        target_selectors = [".htmldisplay", ".module-content", ".content", "#main-content"]
        found_selector = False
        
        for selector in target_selectors:
            if await page.locator(selector).count() > 0:
                # æ’é™¤éš±è—å…ƒç´ 
                elements = await page.locator(f"{selector}:visible").all()
                for el in elements:
                    text = await el.inner_text()
                    if len(text.strip()) > 10: # ç¨å¾®éæ¿¾å¤ªçŸ­çš„é›œè¨Š
                        full_text += text + "\n"
                        
                        # æŠ“é™„ä»¶
                        links = await el.locator("a").all()
                        for link in links:
                            href = await link.get_attribute("href")
                            name = await link.inner_text()
                            if href and any(ext in href.lower() for ext in ['.pdf', '.doc', '.xls', '.ppt']):
                                if href.startswith("/"): href = "https://www.nihs.tp.edu.tw" + href
                                data["attachments"].append({"name": name.strip(), "url": href})
                
                if len(full_text) > 20: # ç¢ºä¿æœ‰æŠ“åˆ°æ±è¥¿
                    found_selector = True
                    break # æ‰¾åˆ°ä¸€ç¨®å°±å¤ äº†

        # ç­–ç•¥ 2: å¦‚æœä¸Šé¢éƒ½æ²’æŠ“åˆ°ï¼Œä½¿ç”¨ã€Œå¤§çµ•æ‹›ã€æŠ“ Body ä¸¦æ¸…æ´—
        if not found_selector or len(full_text) < 20:
            #print(f"{prefix}   âš ï¸ æ¨™æº–å€å¡Šç„¡å…§å®¹ï¼Œå•Ÿå‹•å…¨é æƒæ...")
            full_text = await page.evaluate("""() => {
                // è¤‡è£½ body é¿å…ç ´å£é é¢
                let clone = document.body.cloneNode(true);
                // ç§»é™¤å°è¦½åˆ—ã€é å°¾ã€è…³æœ¬
                let garbages = clone.querySelectorAll('nav, footer, script, style, .nav-Vertical, .header');
                garbages.forEach(el => el.remove());
                return clone.innerText;
            }""")

        # ç°¡å–®æ¸…æ´—
        lines = [line.strip() for line in full_text.split('\n') if line.strip()]
        data["content"] = "\n".join(lines)
        
        if len(data["content"]) > 30: # é–€æª»è¨­ä½ä¸€é»ï¼Œé¿å…æ¼æŠ“
            print(f"{prefix}   ğŸ“ æŠ“åˆ°å…§å®¹: {len(data['content'])} å­—")
            all_data.append(data)
        else:
            print(f"{prefix}   âš ï¸ å…§å®¹éçŸ­æˆ–ç¢ºå¯¦ç‚ºç›®éŒ„é ")

        # --- B. åµæ¸¬å­é¸å–® ---
        # å°‹æ‰¾å·¦å´å°èˆªåˆ—
        sub_links = await page.locator(".nav-Vertical a").all()
        
        next_targets = []
        for link in sub_links:
            href = await link.get_attribute("href")
            name = await link.inner_text()
            name = name.strip()
            
            if href and name:
                if href.startswith("http") and "nihs.tp.edu.tw" not in href: continue
                
                if not href.startswith("http"):
                    if href.startswith("/nss/p/"):
                         full_href = f"https://www.nihs.tp.edu.tw{href}"
                    elif href.startswith("/"):
                         full_href = f"https://www.nihs.tp.edu.tw{href}"
                    else:
                         full_href = f"{BASE_URL}{href}"
                else:
                    full_href = href
                
                if full_href not in visited_urls:
                    next_targets.append((name, full_href))

        if next_targets:
            print(f"{prefix}   ğŸ”— ç™¼ç¾ {len(next_targets)} å€‹å­åˆ†é ...")
            for sub_name, sub_url in next_targets:
                await extract_content(page, category, f"{title}-{sub_name}", sub_url, depth + 1)
                # âœ… ä¿®æ­£ 3: éè¿´é–“éš”ç¨å¾®åŠ é•·ï¼Œæ¸›è¼•ä¼ºæœå™¨è² æ“”
                await asyncio.sleep(random.uniform(1.0, 2.0))

    except Exception as e:
        print(f"{prefix}   âŒ éŒ¯èª¤: {e}")

async def main():
    print("ğŸš€ V44 (å¯¬å®¹æ“·å–ç‰ˆ) å•Ÿå‹•...")
    async with async_playwright() as p:
        # âœ… é›²ç«¯å¿…é ˆæ˜¯ True
        browser = await p.chromium.launch(headless=True) 
        
        # ä½¿ç”¨çœŸå¯¦ User-Agent
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            viewport={"width": 1280, "height": 800} # è¨­å®šè¦–çª—å¤§å°ç¢ºä¿ä¸æœƒè®Šæˆæ‰‹æ©Ÿç‰ˆ
        )
        page = await context.new_page()

        for name, pid in START_PAGES.items():
            start_url = f"{BASE_URL}{pid}"
            await extract_content(page, name, name, start_url)

        await browser.close()

    print("\n" + "="*30)
    if len(all_data) > 0:
        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=4)
        print(f"âœ… å®Œæˆï¼å…±æŠ“å– {len(all_data)} é ã€‚")
    else:
        print("âš ï¸ æœªæŠ“å–åˆ°è³‡æ–™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())
