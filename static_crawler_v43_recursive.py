import os
import json
import time
import random
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
# å¼•å…¥é‡è©¦æ©Ÿåˆ¶
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ==========================================
# ğŸ¯ è¨­å®šå€
# ==========================================
START_URL = "https://www.nihs.tp.edu.tw/nss/p/index"  # å­¸æ ¡é¦–é 
OUTPUT_FILE = "nihs_final_v40.json" # éœæ…‹çˆ¬èŸ²æš«å­˜æª”
MAX_DEPTH = 3  # éè¿´æ·±åº¦ (é¿å…çˆ¬å¤ªæ·±å›ä¸ä¾†)

# âœ… ä¿®æ­£ 1: å®Œæ•´çš„ç€è¦½å™¨å½è£æ¨™é ­
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept-Encoding": "gzip, deflate, br",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0"
}

visited_urls = set()
all_data = []

# ==========================================
# ğŸ› ï¸ å·¥å…·å‡½å¼ï¼šå»ºç«‹å¼·å¥çš„ Session
# ==========================================
def get_session():
    """ å»ºç«‹ä¸€å€‹å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ Session """
    session = requests.Session()
    # è¨­å®šé‡è©¦ç­–ç•¥ï¼šé‡åˆ° 500, 502, 503, 504 éŒ¯èª¤æ™‚ï¼Œæœ€å¤šé‡è©¦ 3 æ¬¡ï¼Œæ¯æ¬¡é–“éš”æ™‚é–“åŠ å€
    retry = Retry(total=3, read=3, connect=3, backoff_factor=2, status_forcelist=[500, 502, 503, 504])
    adapter = HTTPAdapter(max_retries=retry)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update(HEADERS)
    return session

# åˆå§‹åŒ– Session
http_session = get_session()

def clean_text(text):
    """ æ¸…ç†å¤šé¤˜çš„ç©ºç™½èˆ‡æ›è¡Œ """
    if not text:
        return ""
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

def crawl_recursive(url, depth, category="æ ¡åœ’éœæ…‹è³‡è¨Š"):
    """ éè¿´çˆ¬å–å‡½å¼ """
    if depth > MAX_DEPTH:
        return
    
    # å»é™¤åƒæ•¸ï¼Œé¿å…é‡è¤‡çˆ¬å– (ä¾‹å¦‚ ?id=1 èˆ‡ ?id=1&t=2 è¦–ç‚ºåŒä¸€é )
    parsed = urlparse(url)
    clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    
    if clean_url in visited_urls:
        return
    visited_urls.add(clean_url)

    print(f"{'   ' * (3-depth)}ğŸ” åˆ†æé é¢: {url}")

    try:
        # âœ… ä¿®æ­£ 2: åŠ å…¥éš¨æ©Ÿå»¶é² (1~3ç§’)ï¼Œæ¨¡æ“¬äººé¡è¡Œç‚ºï¼Œé¿å…è¢«é›²ç«¯é˜²ç«ç‰†å°é–
        time.sleep(random.uniform(1.5, 3.5))

        response = http_session.get(url, timeout=20) # å»¶é•· timeout
        
        # å¦‚æœç‹€æ…‹ç¢¼ä¸æ˜¯ 200ï¼Œè·³é
        if response.status_code != 200:
            print(f"âš ï¸ ç„¡æ³•è®€å– ({response.status_code})")
            return

        soup = BeautifulSoup(response.text, 'lxml') # å»ºè­°å®‰è£ lxml: pip install lxml

        # 1. æŠ“å–æ¨™é¡Œ
        title = soup.title.string.strip() if soup.title else "ç„¡æ¨™é¡Œ"
        
        # 2. æŠ“å–ä¸»è¦å…§å®¹ (é‡å° NSS ç³»çµ±çµæ§‹å„ªåŒ–)
        # å˜—è©¦æŠ“å–å¸¸è¦‹çš„å…§å®¹å€å¡Š ID æˆ– Class
        content_div = soup.find('div', class_='content') or \
                      soup.find('div', id='main_content') or \
                      soup.find('div', class_='module-content') or \
                      soup.body

        content_text = ""
        attachments = []
        
        if content_div:
            # ç§»é™¤ script, style, nav ç­‰å¹²æ“¾å…ƒç´ 
            for bad in content_div(['script', 'style', 'nav', 'header', 'footer', 'iframe']):
                bad.decompose()
            
            content_text = clean_text(content_div.get_text())
            
            # å˜—è©¦æŠ“å–é™„ä»¶é€£çµ (PDF/Word)
            for a in content_div.find_all('a', href=True):
                href = a['href']
                if href.lower().endswith(('.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx')):
                    full_link = urljoin(url, href)
                    attachments.append({
                        "name": a.get_text(strip=True) or "é™„ä»¶",
                        "url": full_link
                    })

        # åªæœ‰ç•¶å…§å®¹é•·åº¦è¶³å¤ æ™‚æ‰å„²å­˜ (éæ¿¾æ‰ç©ºé é¢æˆ–è¼‰å…¥é )
        if len(content_text) > 50:
            print(f"{'   ' * (3-depth)}   ğŸ“ æŠ“åˆ°å…§å®¹: {len(content_text)} å­—")
            
            all_data.append({
                "category": category,
                "unit": "æ ¡åœ’å®˜ç¶²", # éœæ…‹é é¢è¼ƒé›£åˆ†å–®ä½ï¼Œçµ±ä¸€æ¨™ç¤º
                "date": time.strftime("%Y/%m/%d"), # æŠ“å–ç•¶å¤©æ—¥æœŸ
                "title": title,
                "url": url,
                "content": content_text,
                "attachments": attachments,
                "crawled_at": time.strftime("%Y-%m-%d %H:%M:%S")
            })
        else:
            print(f"{'   ' * (3-depth)}   âš ï¸ å…§å®¹éçŸ­æˆ–ç„¡å…§æ–‡ (å¯èƒ½éœ€ JavaScript æ¸²æŸ“)")

        # 3. ç¹¼çºŒéè¿´æŠ“å–å­é€£çµ (åªæŠ“åŒç¶²åŸŸä¸‹çš„é€£çµ)
        # é‡å°ã€Œé—œæ–¼æ¹–å·¥ã€é€™ç¨®ç›®éŒ„çµæ§‹
        sub_links = []
        # æŠ“å–å·¦å´é¸å–®æˆ–å…§å®¹å€çš„é€£çµ
        target_area = soup.find('div', class_='panel-group') or content_div
        
        if target_area:
            for a in target_area.find_all('a', href=True):
                href = a['href']
                full_link = urljoin(url, href)
                
                # ç°¡å–®éæ¿¾ï¼šåªæŠ“å…§æ¹–é«˜å·¥ç¶²åŸŸï¼Œä¸”ä¸æŠ“åœ–ç‰‡/æª”æ¡ˆ
                if "nihs.tp.edu.tw" in full_link and not href.lower().endswith(('.jpg', '.png', '.pdf', '.zip')):
                    sub_links.append(full_link)

        # å»é‡
        sub_links = list(set(sub_links))
        
        if len(sub_links) > 0:
            print(f"{'   ' * (3-depth)}   ğŸ”— ç™¼ç¾ {len(sub_links)} å€‹å­åˆ†é ï¼Œæº–å‚™æ·±å…¥...")
            
            for link in sub_links:
                crawl_recursive(link, depth + 1, category)

    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤ {url}: {e}")

# ==========================================
# ğŸš€ ä¸»ç¨‹å¼
# ==========================================
if __name__ == "__main__":
    print("ğŸš€ V43 (é›²ç«¯æŠ—åµæ¸¬ç‰ˆ) å•Ÿå‹•...")
    print(f"ğŸ•·ï¸ ç›®æ¨™é¦–é : {START_URL}")
    
    # é–‹å§‹çˆ¬èŸ²
    crawl_recursive(START_URL, 1)
    
    # å­˜æª”
    print(f"ğŸ’¾ çˆ¬å–å®Œæˆï¼Œå…± {len(all_data)} ç­†è³‡æ–™ï¼Œæ­£åœ¨å­˜æª”...")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=4)
    
    print(f"âœ… æª”æ¡ˆå·²å„²å­˜: {OUTPUT_FILE}")
